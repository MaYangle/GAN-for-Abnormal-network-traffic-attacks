import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# =========================
# ✅ CGAN 结构
# =========================
class CGAN_Generator(nn.Module):
    def __init__(self, noise_dim, label_dim, output_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim + label_dim, 512),
            nn.BatchNorm1d(512), nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.BatchNorm1d(1024), nn.LeakyReLU(0.2),
            nn.Linear(1024, output_dim),
            nn.Tanh()
        )

    def forward(self, z, labels):
        x = torch.cat([z, labels], dim=1)
        return self.model(x)

class CGAN_Discriminator(nn.Module):
    def __init__(self, input_dim, label_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim + label_dim, 1024),
            nn.LeakyReLU(0.2), nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2), nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x, labels):
        x = torch.cat([x, labels], dim=1)
        return self.model(x)

def train_cgan(X, y, num_classes, noise_dim=100, epochs=1000, batch_size=64):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = torch.tensor(X, dtype=torch.float32).to(device)
    y = torch.tensor(y, dtype=torch.long).to(device)
    y_onehot = torch.nn.functional.one_hot(y, num_classes).float().to(device)

    G = CGAN_Generator(noise_dim, num_classes, X.shape[1]).to(device)
    D = CGAN_Discriminator(X.shape[1], num_classes).to(device)

    criterion = nn.BCELoss()
    opt_G = optim.Adam(G.parameters(), lr=0.0002)
    opt_D = optim.Adam(D.parameters(), lr=0.0002)

    for epoch in range(epochs):
        idx = torch.randint(0, X.shape[0], (batch_size,))
        real_x = X[idx]
        real_y = y_onehot[idx]

        z = torch.randn(batch_size, noise_dim).to(device)
        gen_labels = torch.zeros(batch_size, dtype=torch.long).to(device)
        gen_onehot = torch.nn.functional.one_hot(gen_labels, num_classes).float().to(device)
        fake_x = G(z, gen_onehot)

        # Train D
        real_score = D(real_x, real_y)
        fake_score = D(fake_x.detach(), gen_onehot)
        loss_D = criterion(real_score, torch.ones_like(real_score)) + criterion(fake_score, torch.zeros_like(fake_score))
        opt_D.zero_grad(); loss_D.backward(); opt_D.step()

        # Train G
        gen_score = D(fake_x, gen_onehot)
        loss_G = criterion(gen_score, torch.ones_like(gen_score))
        opt_G.zero_grad(); loss_G.backward(); opt_G.step()

    return G

# =========================
# ✅ ACGAN 结构
# =========================
class ACGAN_Generator(nn.Module):
    def __init__(self, noise_dim, label_dim, output_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(noise_dim + label_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Linear(1024, output_dim),
            nn.Tanh()
        )

    def forward(self, z, labels):
        x = torch.cat([z, labels], dim=1)
        return self.model(x)

class ACGAN_Discriminator(nn.Module):
    def __init__(self, input_dim, label_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
        )
        self.adv = nn.Sequential(nn.Linear(256, 1), nn.Sigmoid())
        self.cls = nn.Sequential(nn.Linear(256, label_dim), nn.LogSoftmax(dim=1))

    def forward(self, x):
        features = self.shared(x)
        validity = self.adv(features)
        label_pred = self.cls(features)
        return validity, label_pred

def train_acgan(X, y, num_classes, noise_dim=100, epochs=1000, batch_size=64):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = torch.tensor(X, dtype=torch.float32).to(device)
    y = torch.tensor(y, dtype=torch.long).to(device)

    G = ACGAN_Generator(noise_dim, num_classes, X.shape[1]).to(device)
    D = ACGAN_Discriminator(X.shape[1], num_classes).to(device)

    opt_G = optim.Adam(G.parameters(), lr=0.0002)
    opt_D = optim.Adam(D.parameters(), lr=0.0002)
    adv_loss = nn.BCELoss()
    cls_loss = nn.NLLLoss()

    for epoch in range(epochs):
        idx = torch.randint(0, X.shape[0], (batch_size,))
        real_x = X[idx]
        real_y = y[idx]
        real_y_onehot = torch.nn.functional.one_hot(real_y, num_classes).float().to(device)

        real_validity, real_cls = D(real_x)
        d_loss_real = adv_loss(real_validity, torch.ones_like(real_validity)) + cls_loss(real_cls, real_y)

        z = torch.randn(batch_size, noise_dim).to(device)
        fake_labels = torch.zeros(batch_size, dtype=torch.long).to(device)
        fake_onehot = torch.nn.functional.one_hot(fake_labels, num_classes).float().to(device)
        fake_x = G(z, fake_onehot)

        fake_validity, fake_cls = D(fake_x.detach())
        d_loss_fake = adv_loss(fake_validity, torch.zeros_like(fake_validity)) + cls_loss(fake_cls, fake_labels)

        d_loss = d_loss_real + d_loss_fake
        opt_D.zero_grad(); d_loss.backward(); opt_D.step()

        gen_validity, gen_cls = D(fake_x)
        g_loss = adv_loss(gen_validity, torch.ones_like(gen_validity)) + cls_loss(gen_cls, fake_labels)
        opt_G.zero_grad(); g_loss.backward(); opt_G.step()

    return G

# =========================
# ✅ 生成样本通用函数
# =========================
def generate_samples(G, target_class, num_classes, n_samples=500, noise_dim=100):
    G.eval()
    device = next(G.parameters()).device
    z = torch.randn(n_samples, noise_dim).to(device)
    labels = torch.full((n_samples,), target_class, dtype=torch.long).to(device)
    one_hot = torch.nn.functional.one_hot(labels, num_classes).float().to(device)
    with torch.no_grad():
        samples = G(z, one_hot).cpu().numpy()
    return samples

# =========================
# ✅ 高质量 CGAN 增强接口
# =========================
def cgan_augment(df, rare_classes, gen_config, feature_names=None):
    all_generated = []
    for label in rare_classes:
        class_df = df[df["Label"] == label].copy()
        if len(class_df) < 5:
            print(f"⚠️ 类别 {label} 样本太少，跳过")
            continue
        class_df["Label_enc"] = 0
        X = class_df.drop(columns=["Label", "Label_enc"])
        feature_names = X.columns.tolist() if feature_names is None else feature_names
        X = X[feature_names].apply(pd.to_numeric, errors='coerce').fillna(0).values
        y = class_df["Label_enc"].values
        G = train_cgan(X, y, num_classes=1)
        synthetic = generate_samples(G, 0, 1, gen_config[label])
        syn_df = pd.DataFrame(synthetic, columns=feature_names)
        syn_df["Label"] = label
        all_generated.append(syn_df)
    gen_df = pd.concat(all_generated, ignore_index=True)
    return pd.concat([df, gen_df], ignore_index=True), gen_df

# =========================
# ✅ 高质量 ACGAN 增强接口
# =========================
def acgan_augment(df, rare_classes, gen_config, feature_names=None):
    all_generated = []
    for label in rare_classes:
        class_df = df[df["Label"] == label].copy()
        if len(class_df) < 5:
            continue
        class_df["Label_enc"] = 0
        X = class_df.drop(columns=["Label", "Label_enc"])
        feature_names = X.columns.tolist() if feature_names is None else feature_names
        X = X[feature_names].apply(pd.to_numeric, errors='coerce').fillna(0).values
        y = class_df["Label_enc"].values
        G = train_acgan(X, y, num_classes=1)
        synthetic = generate_samples(G, 0, 1, gen_config[label])
        syn_df = pd.DataFrame(synthetic, columns=feature_names)
        syn_df["Label"] = label
        all_generated.append(syn_df)
    gen_df = pd.concat(all_generated, ignore_index=True)
    return pd.concat([df, gen_df], ignore_index=True), gen_df
