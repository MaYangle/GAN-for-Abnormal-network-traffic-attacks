{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# è¯»å–é¢„å¤„ç†åçš„æ•°æ®\n",
    "df = pd.read_csv(r'C:\\\\Users\\\\æˆ´å°”\\\\Desktop\\\\sc\\\\sc\\\\Datasets/processed_total.csv')\n",
    "\n",
    "# ç­›é€‰ç›®æ ‡ç±»åˆ«ï¼ˆä¸åŸå§‹ä»£ç ä¸€è‡´ï¼‰\n",
    "rare_classes = [\"Infiltration\", \"Web Attack ï¿½ XSS\", \"Web Attack ï¿½ Sql Injection\", \"Heartbleed\"]\n",
    "benign_df = df[df[\"Label\"] == \"BENIGN\"].sample(n=100000, random_state=42)\n",
    "target_df = df[df[\"Label\"].isin(rare_classes)]\n",
    "small_df = pd.concat([benign_df, target_df], ignore_index=True)\n",
    "\n",
    "# æ‹†åˆ†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "X = small_df.drop(columns=[\"Label\"])\n",
    "y = small_df[\"Label\"]\n",
    "\n",
    "# å°†æ ‡ç­¾ç¼–ç ä¸ºæ•°å€¼\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, stratify=y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorch Tensor\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# åˆ›å»ºDataLoader\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TabularDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TabularDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=128, nhead=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # æ–°å¢ï¼šæ·»åŠ CLS Tokenç”¨äºåˆ†ç±»\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))  # æ–°å¢CLS Token\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # ç‰¹å¾åµŒå…¥\n",
    "        x = self.feature_embedding(x)  # (batch_size, num_features) â†’ (batch_size, d_model)\n",
    "        # æ·»åŠ CLS Tokenåˆ°åºåˆ—ä¸­\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (batch_size, seq_len=2, d_model)\n",
    "        # Transformerå¤„ç†\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # æå–CLS Tokenä½œä¸ºåˆ†ç±»ç‰¹å¾\n",
    "        cls_output = x[0]  # (batch_size, d_model)\n",
    "        # åˆ†ç±»\n",
    "        x = self.classifier(cls_output)\n",
    "        return x\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = FTTransformer(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_model(model, train_loader, epochs=20):\n",
    "    print(\"å¼€å§‹è®­ç»ƒ.....\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            print(f\"batch_y ä¸­çš„æœ€å¤§å€¼: {batch_y.max().item()}\")\n",
    "            print(f\"num_classes: {num_classes}\")\n",
    "            assert batch_y.max().item() < num_classes, \"é”™è¯¯: batch_y ä¸­åŒ…å«è¶…å‡º num_classes èŒƒå›´çš„å€¼.\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            print(np.shape(batch_x))\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    print(\"ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    print(\"ğŸ“‰ æ··æ·†çŸ©é˜µ:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹è®­ç»ƒ.....\n",
      "Epoch 1/20, Loss: 0.0403\n",
      "Epoch 2/20, Loss: 0.0232\n",
      "Epoch 3/20, Loss: 0.0225\n",
      "Epoch 4/20, Loss: 0.0268\n",
      "Epoch 5/20, Loss: 0.0237\n",
      "Epoch 6/20, Loss: 0.0212\n",
      "Epoch 7/20, Loss: 0.0238\n",
      "Epoch 8/20, Loss: 0.0194\n",
      "Epoch 9/20, Loss: 0.0220\n",
      "Epoch 10/20, Loss: 0.0207\n",
      "Epoch 11/20, Loss: 0.0200\n",
      "Epoch 12/20, Loss: 0.0213\n",
      "Epoch 13/20, Loss: 0.0198\n",
      "Epoch 14/20, Loss: 0.0194\n",
      "Epoch 15/20, Loss: 0.0193\n",
      "Epoch 16/20, Loss: 0.0227\n",
      "Epoch 17/20, Loss: 0.0196\n",
      "Epoch 18/20, Loss: 0.0215\n",
      "Epoch 19/20, Loss: 0.0205\n",
      "Epoch 20/20, Loss: 0.0232\n"
     ]
    }
   ],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    BENIGN       0.99      1.00      1.00     20000\n",
      "                Heartbleed       0.00      0.00      0.00         2\n",
      "              Infiltration       0.00      0.00      0.00         7\n",
      "Web Attack ï¿½ Sql Injection       0.00      0.00      0.00         4\n",
      "          Web Attack ï¿½ XSS       0.00      0.00      0.00       131\n",
      "\n",
      "                  accuracy                           0.99     20144\n",
      "                 macro avg       0.20      0.20      0.20     20144\n",
      "              weighted avg       0.99      0.99      0.99     20144\n",
      "\n",
      "ğŸ“‰ æ··æ·†çŸ©é˜µ:\n",
      "[[20000     0     0     0     0]\n",
      " [    2     0     0     0     0]\n",
      " [    7     0     0     0     0]\n",
      " [    4     0     0     0     0]\n",
      " [  131     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# è¯„ä¼°æ¨¡å‹\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª å½“å‰ç±»åˆ«: Web Attack ï¿½ XSSï¼ŒçœŸå®æ ·æœ¬æ•°é‡: 652\n",
      "ğŸš€ è®­ç»ƒ CTGAN (ä½¿ç”¨ 100 æ¡çœŸå®æ ·æœ¬)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (0.00) | Discrim. (0.00):   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Gen. (0.65) | Discrim. (-0.54): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:51<00:00,  5.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å°†ç”Ÿæˆ 500 æ¡å¢å¼ºæ ·æœ¬ã€‚\n",
      "âœ… å¢å¼ºå®Œæˆ: Web Attack ï¿½ XSS â†’ 500 æ¡æ ·æœ¬\n",
      "\n",
      "ğŸ§ª å½“å‰ç±»åˆ«: Infiltrationï¼ŒçœŸå®æ ·æœ¬æ•°é‡: 36\n",
      "ğŸš€ è®­ç»ƒ CTGAN (ä½¿ç”¨ 36 æ¡çœŸå®æ ·æœ¬)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-2.27) | Discrim. (-0.62): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:51<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å°†ç”Ÿæˆ 180 æ¡å¢å¼ºæ ·æœ¬ã€‚\n",
      "âœ… å¢å¼ºå®Œæˆ: Infiltration â†’ 180 æ¡æ ·æœ¬\n",
      "\n",
      "ğŸ§ª å½“å‰ç±»åˆ«: Web Attack ï¿½ Sql Injectionï¼ŒçœŸå®æ ·æœ¬æ•°é‡: 21\n",
      "ğŸš€ è®­ç»ƒ CTGAN (ä½¿ç”¨ 21 æ¡çœŸå®æ ·æœ¬)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (5.36) | Discrim. (0.34): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:50<00:00,  5.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å°†ç”Ÿæˆ 105 æ¡å¢å¼ºæ ·æœ¬ã€‚\n",
      "âœ… å¢å¼ºå®Œæˆ: Web Attack ï¿½ Sql Injection â†’ 105 æ¡æ ·æœ¬\n",
      "\n",
      "ğŸ§ª å½“å‰ç±»åˆ«: Heartbleedï¼ŒçœŸå®æ ·æœ¬æ•°é‡: 11\n",
      "ğŸš€ è®­ç»ƒ CTGAN (ä½¿ç”¨ 11 æ¡çœŸå®æ ·æœ¬)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (2.95) | Discrim. (0.02): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:50<00:00,  5.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å°†ç”Ÿæˆ 50 æ¡å¢å¼ºæ ·æœ¬ã€‚\n",
      "âœ… å¢å¼ºå®Œæˆ: Heartbleed â†’ 50 æ¡æ ·æœ¬\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰ç±»åˆ«å¢å¼ºå®Œæ¯•ï¼Œæ€»å…±ç”Ÿæˆæ ·æœ¬æ•°: 835\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "\n",
    "# âœ… è®¾å®šç¨€æœ‰ç±»æ ‡ç­¾åˆ—è¡¨\n",
    "rare_classes = [\n",
    "    \"Web Attack ï¿½ XSS\",\n",
    "    \"Infiltration\",\n",
    "    \"Web Attack ï¿½ Sql Injection\",\n",
    "    \"Heartbleed\"\n",
    "]\n",
    "\n",
    "# âœ… è®¾ç½®å¢å¼ºå‚æ•°\n",
    "min_real_threshold = 10      # æœ€å°‘å¤šå°‘æ¡æ ·æœ¬æ‰å…è®¸è®­ç»ƒ GAN\n",
    "max_real_sample = 100        # æ¯ç±»æœ€å¤šä½¿ç”¨å¤šå°‘æ¡çœŸå®æ ·æœ¬è®­ç»ƒ GAN\n",
    "default_generate_n = 500     # é»˜è®¤ç”Ÿæˆæ•°é‡\n",
    "scaling_ratio = 5            # æ¯ä¸ªçœŸå®æ ·æœ¬æ‰©å¢å¤šå°‘å€\n",
    "\n",
    "# âœ… åˆå§‹åŒ–ç”Ÿæˆç»“æœå®¹å™¨\n",
    "synthetic_samples_list = []\n",
    "\n",
    "for category in rare_classes:\n",
    "    # æå–è¯¥ç±»åˆ«æ‰€æœ‰çœŸå®æ ·æœ¬\n",
    "    category_df = df[df[\"Label\"] == category].copy()\n",
    "    available_n = len(category_df)\n",
    "\n",
    "    print(f\"ğŸ§ª å½“å‰ç±»åˆ«: {category}ï¼ŒçœŸå®æ ·æœ¬æ•°é‡: {available_n}\")\n",
    "\n",
    "    if available_n < min_real_threshold:\n",
    "        print(f\"âš ï¸ æ ·æœ¬æ•°è¿‡å°‘ (<{min_real_threshold})ï¼Œè·³è¿‡è¯¥ç±»å¢å¼ºã€‚\\n\")\n",
    "        continue\n",
    "\n",
    "    # é™åˆ¶æœ€å¤§è®­ç»ƒæ•°é‡\n",
    "    train_n = min(max_real_sample, available_n)\n",
    "    real_samples = category_df.sample(n=train_n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # è·å–ç‰¹å¾åˆ—\n",
    "    features = real_samples.drop(columns=[\"Label\"])\n",
    "    discrete_columns = features.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    # åˆå§‹åŒ–å¹¶è®­ç»ƒ CTGAN\n",
    "    print(f\"ğŸš€ è®­ç»ƒ CTGAN (ä½¿ç”¨ {train_n} æ¡çœŸå®æ ·æœ¬)...\")\n",
    "    ctgan = CTGAN(epochs=300, verbose=True)\n",
    "    ctgan.fit(features, discrete_columns=discrete_columns)\n",
    "\n",
    "    # ğŸ§  è®¾å®šç”Ÿæˆæ ·æœ¬æ•°é‡ï¼šHeartbleed ç‰¹åˆ«å¤„ç†\n",
    "    if category == \"Heartbleed\":\n",
    "        generate_n = 50\n",
    "    else:\n",
    "        generate_n = min(default_generate_n, train_n * scaling_ratio)\n",
    "\n",
    "    print(f\"ğŸ¯ å°†ç”Ÿæˆ {generate_n} æ¡å¢å¼ºæ ·æœ¬ã€‚\")\n",
    "\n",
    "    # ç”Ÿæˆæ ·æœ¬\n",
    "    synthetic = ctgan.sample(generate_n)\n",
    "    synthetic[\"Label\"] = category\n",
    "    synthetic.columns = features.columns.tolist() + [\"Label\"]\n",
    "\n",
    "    # æ·»åŠ è¿›æ€»åˆ—è¡¨\n",
    "    synthetic_samples_list.append(synthetic)\n",
    "    print(f\"âœ… å¢å¼ºå®Œæˆ: {category} â†’ {generate_n} æ¡æ ·æœ¬\\n\")\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰å¢å¼ºæ ·æœ¬\n",
    "final_synthetic_data = pd.concat(synthetic_samples_list, ignore_index=True)\n",
    "print(f\"ğŸ‰ æ‰€æœ‰ç±»åˆ«å¢å¼ºå®Œæ¯•ï¼Œæ€»å…±ç”Ÿæˆæ ·æœ¬æ•°: {len(final_synthetic_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Web Attack ï¿½ XSS              500\n",
      "Infiltration                  180\n",
      "Web Attack ï¿½ Sql Injection    105\n",
      "Heartbleed                     50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_synthetic_data[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN                        10000\n",
      "Web Attack ï¿½ XSS                500\n",
      "Infiltration                    180\n",
      "Web Attack ï¿½ Sql Injection      105\n",
      "Heartbleed                       61\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 15\n",
      "å¼€å§‹è®­ç»ƒ.....\n",
      "Epoch 1/20, Loss: 0.0458\n",
      "Epoch 2/20, Loss: 0.0234\n",
      "Epoch 3/20, Loss: 0.0220\n",
      "Epoch 4/20, Loss: 0.0229\n",
      "Epoch 5/20, Loss: 0.0235\n",
      "Epoch 6/20, Loss: 0.0203\n",
      "Epoch 7/20, Loss: 0.0193\n",
      "Epoch 8/20, Loss: 0.0212\n",
      "Epoch 9/20, Loss: 0.0232\n",
      "Epoch 10/20, Loss: 0.0252\n",
      "Epoch 11/20, Loss: 0.0212\n",
      "Epoch 12/20, Loss: 0.0206\n",
      "Epoch 13/20, Loss: 0.0192\n",
      "Epoch 14/20, Loss: 0.0215\n",
      "Epoch 15/20, Loss: 0.0220\n",
      "Epoch 16/20, Loss: 0.0184\n",
      "Epoch 17/20, Loss: 0.0204\n",
      "Epoch 18/20, Loss: 0.0228\n",
      "Epoch 19/20, Loss: 0.0229\n",
      "Epoch 20/20, Loss: 0.0251\n"
     ]
    }
   ],
   "source": [
    "# åŠ å…¥çœŸå®æ ·æœ¬ï¼ˆæ¯ç±»å°½é‡ä¿ç•™å…¨éƒ¨ï¼‰å’Œ BENIGN æ ·æœ¬\n",
    "benign_df = df[df[\"Label\"] == \"BENIGN\"].sample(n=10000, random_state=42)\n",
    "real_rare_df = df[df[\"Label\"] == \"Heartbleed\"] # æ‰€æœ‰çœŸå®ç¨€æœ‰ç±»æ ·æœ¬\n",
    "\n",
    "# åˆå¹¶è®­ç»ƒé›†\n",
    "train_df = pd.concat([benign_df, real_rare_df, final_synthetic_data], ignore_index=True)\n",
    "\n",
    "# æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ\n",
    "print(train_df[\"Label\"].value_counts())\n",
    "\n",
    "# ç‰¹å¾å’Œæ ‡ç­¾æ‹†åˆ†\n",
    "X = small_df.drop(columns=[\"Label\"])\n",
    "y = small_df[\"Label\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"Label\"])  # ä½¿ç”¨å®Œæ•´æ•°æ®é›†é€‚é…ç¼–ç å™¨\n",
    "\n",
    "\n",
    "# æ•°æ®åˆ’åˆ†\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ‡ç­¾è¿›è¡Œç¼–ç \n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train1)\n",
    "y_test_encoded = label_encoder.transform(y_test1)\n",
    "# è®­ç»ƒ baseline æ¨¡å‹ï¼ˆFT transformerï¼‰\n",
    "X_train_tensor1 = torch.tensor(X_train1.values, dtype=torch.float32)\n",
    "y_train_tensor1 = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor1 = torch.tensor(X_test1.values, dtype=torch.float32)\n",
    "y_test_tensor1 = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# åˆ›å»ºDataLoader\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset1 = TabularDataset(X_train_tensor1, y_train_tensor1)\n",
    "test_dataset1 = TabularDataset(X_test_tensor1, y_test_tensor1)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
    "num_features = X_train1.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = FTTransformer(num_features, num_classes)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# æ£€æŸ¥æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾'\n",
    "print(f\"num_classes: {num_classes}\")\n",
    "train_model(model, train_loader1, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (606979458.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31munique_labels=[0 8 9 13 14]\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------- æ ‡ç­¾ç¼–ç  ----------\n",
    "# åˆå¹¶æ‰€æœ‰æ ‡ç­¾ï¼ˆåŒ…æ‹¬åˆæˆæ•°æ®ï¼‰\n",
    "all_labels = pd.concat([train_df[\"Label\"], final_synthetic_data[\"Label\"]], ignore_index=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)  # é€‚é…æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†æ ‡ç­¾ç¼–ç \n",
    "y_encoded = label_encoder.transform(train_df[\"Label\"])\n",
    "\n",
    "# éªŒè¯æ ‡ç­¾èŒƒå›´\n",
    "unique_labels = np.unique(y_encoded)\n",
    "unique_labels=[0, 8 ,9, 13 ,14]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"å”¯ä¸€æ ‡ç­¾å€¼:\", unique_labels)\n",
    "print(\"æ¨¡å‹è¾“å‡ºç±»åˆ«æ•°:\", num_classes)\n",
    "assert unique_labels.max() < num_classes, \"âŒ æ ‡ç­¾å€¼è¶…å‡ºæ¨¡å‹è¾“å‡ºèŒƒå›´\"\n",
    "\n",
    "# ---------- æ¨¡å‹åˆå§‹åŒ– ----------\n",
    "model = FTTransformer(num_features, num_classes)\n",
    "model = model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "\n",
    "# ---------- æ•°æ®åŠ è½½ ----------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.features[idx].to(device),  # è‡ªåŠ¨ç§»è‡³è®¾å¤‡\n",
    "            self.labels[idx].to(device)\n",
    "        )\n",
    "\n",
    "# åˆ›å»º DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LabelEncoder è¯†åˆ«çš„ç±»åˆ«: ['BENIGN' 'Heartbleed' 'Infiltration' 'Web Attack ï¿½ Sql Injection'\n",
      " 'Web Attack ï¿½ XSS']\n",
      "âœ… LabelEncoder è¯†åˆ«çš„ç±»åˆ«æ•°: 5\n",
      "âš ï¸ y_true çœŸå®æ ‡ç­¾ç±»åˆ«: {np.int64(0), np.int64(8), np.int64(9), np.int64(13), np.int64(14)}\n",
      "âš ï¸ y_pred é¢„æµ‹çš„å”¯ä¸€ç±»åˆ«: {np.int64(2), np.int64(4)}\n",
      "ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 5. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“‰ æ··æ·†çŸ©é˜µ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, label_encoder)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mæ¨¡å‹é¢„æµ‹å‡º \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(invalid_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mï¼Œä½†åº”åœ¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m èŒƒå›´å†…ï¼\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“Š åˆ†ç±»æŠ¥å‘Š:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“‰ æ··æ·†çŸ©é˜µ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\æˆ´å°”\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2693\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2687\u001b[39m         warnings.warn(\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2689\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2690\u001b[39m             )\n\u001b[32m   2691\u001b[39m         )\n\u001b[32m   2692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2693\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2697\u001b[39m         )\n\u001b[32m   2698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2699\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 7, does not match size of target_names, 5. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, label_encoder):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # ğŸš¨ æ£€æŸ¥ç±»åˆ«èŒƒå›´\n",
    "    print(\"âœ… LabelEncoder è¯†åˆ«çš„ç±»åˆ«:\", label_encoder.classes_)\n",
    "    print(\"âœ… LabelEncoder è¯†åˆ«çš„ç±»åˆ«æ•°:\", len(label_encoder.classes_))\n",
    "    print(\"âš ï¸ y_true çœŸå®æ ‡ç­¾ç±»åˆ«:\", set(y_true))\n",
    "    print(\"âš ï¸ y_pred é¢„æµ‹çš„å”¯ä¸€ç±»åˆ«:\", set(y_pred))\n",
    "\n",
    "    # ğŸš¨ å¼ºåˆ¶æ£€æŸ¥ y_pred æ˜¯å¦è¶…å‡ºèŒƒå›´\n",
    "    valid_labels = list(range(len(label_encoder.classes_)))  # [0, 1, 2, 3, 4]\n",
    "    invalid_preds = [label for label in y_pred if label not in valid_labels]\n",
    "\n",
    "    if invalid_preds:\n",
    "        print(f\"âŒ å‘ç° {len(invalid_preds)} ä¸ªæ— æ•ˆé¢„æµ‹ç±»åˆ«: {set(invalid_preds)}\")\n",
    "        raise ValueError(f\"æ¨¡å‹é¢„æµ‹å‡º {set(invalid_preds)}ï¼Œä½†åº”åœ¨ {valid_labels} èŒƒå›´å†…ï¼\")\n",
    "\n",
    "    print(\"ğŸ“Š åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    print(\"ğŸ“‰ æ··æ·†çŸ©é˜µ:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "evaluate_model(model, test_loader1, label_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# åˆ›å»º SHAP è§£é‡Šå™¨\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# è®¡ç®— SHAP å€¼\n",
    "shap_values = explainer(X_train1,check_additivity=False)\n",
    "\n",
    "# å¯è§†åŒ– SHAP å€¼ï¼ˆå…¨å±€ï¼‰\n",
    "shap.summary_plot(shap_values, X_train1) \n",
    "# å¯è§†åŒ– SHAP å€¼ï¼ˆå±€éƒ¨ï¼‰\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_train1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
