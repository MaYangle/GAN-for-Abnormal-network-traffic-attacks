{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取预处理后的数据\n",
    "df = pd.read_csv(r'C:\\\\Users\\\\戴尔\\\\Desktop\\\\sc\\\\sc\\\\Datasets/processed_total.csv')\n",
    "\n",
    "# 筛选目标类别（与原始代码一致）\n",
    "rare_classes = [\"Infiltration\", \"Web Attack � XSS\", \"Web Attack � Sql Injection\", \"Heartbleed\"]\n",
    "benign_df = df[df[\"Label\"] == \"BENIGN\"].sample(n=100000, random_state=42)\n",
    "target_df = df[df[\"Label\"].isin(rare_classes)]\n",
    "small_df = pd.concat([benign_df, target_df], ignore_index=True)\n",
    "\n",
    "# 拆分特征和标签\n",
    "X = small_df.drop(columns=[\"Label\"])\n",
    "y = small_df[\"Label\"]\n",
    "\n",
    "# 将标签编码为数值\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, stratify=y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为PyTorch Tensor\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建DataLoader\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = TabularDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TabularDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, d_model=128, nhead=8, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.feature_embedding = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, dropout=0.1\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # 新增：添加CLS Token用于分类\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))  # 新增CLS Token\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # 特征嵌入\n",
    "        x = self.feature_embedding(x)  # (batch_size, num_features) → (batch_size, d_model)\n",
    "        # 添加CLS Token到序列中\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x.unsqueeze(1)), dim=1)  # (batch_size, seq_len=2, d_model)\n",
    "        # Transformer处理\n",
    "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # 提取CLS Token作为分类特征\n",
    "        cls_output = x[0]  # (batch_size, d_model)\n",
    "        # 分类\n",
    "        x = self.classifier(cls_output)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = FTTransformer(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_model(model, train_loader, epochs=20):\n",
    "    print(\"开始训练.....\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            print(f\"batch_y 中的最大值: {batch_y.max().item()}\")\n",
    "            print(f\"num_classes: {num_classes}\")\n",
    "            assert batch_y.max().item() < num_classes, \"错误: batch_y 中包含超出 num_classes 范围的值.\"\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            print(np.shape(batch_x))\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    print(\"📊 分类报告:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    print(\"📉 混淆矩阵:\")\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练.....\n",
      "Epoch 1/20, Loss: 0.0403\n",
      "Epoch 2/20, Loss: 0.0232\n",
      "Epoch 3/20, Loss: 0.0225\n",
      "Epoch 4/20, Loss: 0.0268\n",
      "Epoch 5/20, Loss: 0.0237\n",
      "Epoch 6/20, Loss: 0.0212\n",
      "Epoch 7/20, Loss: 0.0238\n",
      "Epoch 8/20, Loss: 0.0194\n",
      "Epoch 9/20, Loss: 0.0220\n",
      "Epoch 10/20, Loss: 0.0207\n",
      "Epoch 11/20, Loss: 0.0200\n",
      "Epoch 12/20, Loss: 0.0213\n",
      "Epoch 13/20, Loss: 0.0198\n",
      "Epoch 14/20, Loss: 0.0194\n",
      "Epoch 15/20, Loss: 0.0193\n",
      "Epoch 16/20, Loss: 0.0227\n",
      "Epoch 17/20, Loss: 0.0196\n",
      "Epoch 18/20, Loss: 0.0215\n",
      "Epoch 19/20, Loss: 0.0205\n",
      "Epoch 20/20, Loss: 0.0232\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train_model(model, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 分类报告:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "                    BENIGN       0.99      1.00      1.00     20000\n",
      "                Heartbleed       0.00      0.00      0.00         2\n",
      "              Infiltration       0.00      0.00      0.00         7\n",
      "Web Attack � Sql Injection       0.00      0.00      0.00         4\n",
      "          Web Attack � XSS       0.00      0.00      0.00       131\n",
      "\n",
      "                  accuracy                           0.99     20144\n",
      "                 macro avg       0.20      0.20      0.20     20144\n",
      "              weighted avg       0.99      0.99      0.99     20144\n",
      "\n",
      "📉 混淆矩阵:\n",
      "[[20000     0     0     0     0]\n",
      " [    2     0     0     0     0]\n",
      " [    7     0     0     0     0]\n",
      " [    4     0     0     0     0]\n",
      " [  131     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 评估模型\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 当前类别: Web Attack � XSS，真实样本数量: 652\n",
      "🚀 训练 CTGAN (使用 100 条真实样本)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (0.00) | Discrim. (0.00):   0%|          | 0/300 [00:00<?, ?it/s]c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Gen. (0.65) | Discrim. (-0.54): 100%|██████████| 300/300 [00:51<00:00,  5.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 将生成 500 条增强样本。\n",
      "✅ 增强完成: Web Attack � XSS → 500 条样本\n",
      "\n",
      "🧪 当前类别: Infiltration，真实样本数量: 36\n",
      "🚀 训练 CTGAN (使用 36 条真实样本)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (-2.27) | Discrim. (-0.62): 100%|██████████| 300/300 [00:51<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 将生成 180 条增强样本。\n",
      "✅ 增强完成: Infiltration → 180 条样本\n",
      "\n",
      "🧪 当前类别: Web Attack � Sql Injection，真实样本数量: 21\n",
      "🚀 训练 CTGAN (使用 21 条真实样本)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (5.36) | Discrim. (0.34): 100%|██████████| 300/300 [00:50<00:00,  5.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 将生成 105 条增强样本。\n",
      "✅ 增强完成: Web Attack � Sql Injection → 105 条样本\n",
      "\n",
      "🧪 当前类别: Heartbleed，真实样本数量: 11\n",
      "🚀 训练 CTGAN (使用 11 条真实样本)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gen. (2.95) | Discrim. (0.02): 100%|██████████| 300/300 [00:50<00:00,  5.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 将生成 50 条增强样本。\n",
      "✅ 增强完成: Heartbleed → 50 条样本\n",
      "\n",
      "🎉 所有类别增强完毕，总共生成样本数: 835\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ctgan import CTGAN\n",
    "\n",
    "# ✅ 设定稀有类标签列表\n",
    "rare_classes = [\n",
    "    \"Web Attack � XSS\",\n",
    "    \"Infiltration\",\n",
    "    \"Web Attack � Sql Injection\",\n",
    "    \"Heartbleed\"\n",
    "]\n",
    "\n",
    "# ✅ 设置增强参数\n",
    "min_real_threshold = 10      # 最少多少条样本才允许训练 GAN\n",
    "max_real_sample = 100        # 每类最多使用多少条真实样本训练 GAN\n",
    "default_generate_n = 500     # 默认生成数量\n",
    "scaling_ratio = 5            # 每个真实样本扩增多少倍\n",
    "\n",
    "# ✅ 初始化生成结果容器\n",
    "synthetic_samples_list = []\n",
    "\n",
    "for category in rare_classes:\n",
    "    # 提取该类别所有真实样本\n",
    "    category_df = df[df[\"Label\"] == category].copy()\n",
    "    available_n = len(category_df)\n",
    "\n",
    "    print(f\"🧪 当前类别: {category}，真实样本数量: {available_n}\")\n",
    "\n",
    "    if available_n < min_real_threshold:\n",
    "        print(f\"⚠️ 样本数过少 (<{min_real_threshold})，跳过该类增强。\\n\")\n",
    "        continue\n",
    "\n",
    "    # 限制最大训练数量\n",
    "    train_n = min(max_real_sample, available_n)\n",
    "    real_samples = category_df.sample(n=train_n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # 获取特征列\n",
    "    features = real_samples.drop(columns=[\"Label\"])\n",
    "    discrete_columns = features.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    # 初始化并训练 CTGAN\n",
    "    print(f\"🚀 训练 CTGAN (使用 {train_n} 条真实样本)...\")\n",
    "    ctgan = CTGAN(epochs=300, verbose=True)\n",
    "    ctgan.fit(features, discrete_columns=discrete_columns)\n",
    "\n",
    "    # 🧠 设定生成样本数量：Heartbleed 特别处理\n",
    "    if category == \"Heartbleed\":\n",
    "        generate_n = 50\n",
    "    else:\n",
    "        generate_n = min(default_generate_n, train_n * scaling_ratio)\n",
    "\n",
    "    print(f\"🎯 将生成 {generate_n} 条增强样本。\")\n",
    "\n",
    "    # 生成样本\n",
    "    synthetic = ctgan.sample(generate_n)\n",
    "    synthetic[\"Label\"] = category\n",
    "    synthetic.columns = features.columns.tolist() + [\"Label\"]\n",
    "\n",
    "    # 添加进总列表\n",
    "    synthetic_samples_list.append(synthetic)\n",
    "    print(f\"✅ 增强完成: {category} → {generate_n} 条样本\\n\")\n",
    "\n",
    "# 合并所有增强样本\n",
    "final_synthetic_data = pd.concat(synthetic_samples_list, ignore_index=True)\n",
    "print(f\"🎉 所有类别增强完毕，总共生成样本数: {len(final_synthetic_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Web Attack � XSS              500\n",
      "Infiltration                  180\n",
      "Web Attack � Sql Injection    105\n",
      "Heartbleed                     50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_synthetic_data[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN                        10000\n",
      "Web Attack � XSS                500\n",
      "Infiltration                    180\n",
      "Web Attack � Sql Injection      105\n",
      "Heartbleed                       61\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 15\n",
      "开始训练.....\n",
      "Epoch 1/20, Loss: 0.0458\n",
      "Epoch 2/20, Loss: 0.0234\n",
      "Epoch 3/20, Loss: 0.0220\n",
      "Epoch 4/20, Loss: 0.0229\n",
      "Epoch 5/20, Loss: 0.0235\n",
      "Epoch 6/20, Loss: 0.0203\n",
      "Epoch 7/20, Loss: 0.0193\n",
      "Epoch 8/20, Loss: 0.0212\n",
      "Epoch 9/20, Loss: 0.0232\n",
      "Epoch 10/20, Loss: 0.0252\n",
      "Epoch 11/20, Loss: 0.0212\n",
      "Epoch 12/20, Loss: 0.0206\n",
      "Epoch 13/20, Loss: 0.0192\n",
      "Epoch 14/20, Loss: 0.0215\n",
      "Epoch 15/20, Loss: 0.0220\n",
      "Epoch 16/20, Loss: 0.0184\n",
      "Epoch 17/20, Loss: 0.0204\n",
      "Epoch 18/20, Loss: 0.0228\n",
      "Epoch 19/20, Loss: 0.0229\n",
      "Epoch 20/20, Loss: 0.0251\n"
     ]
    }
   ],
   "source": [
    "# 加入真实样本（每类尽量保留全部）和 BENIGN 样本\n",
    "benign_df = df[df[\"Label\"] == \"BENIGN\"].sample(n=10000, random_state=42)\n",
    "real_rare_df = df[df[\"Label\"] == \"Heartbleed\"] # 所有真实稀有类样本\n",
    "\n",
    "# 合并训练集\n",
    "train_df = pd.concat([benign_df, real_rare_df, final_synthetic_data], ignore_index=True)\n",
    "\n",
    "# 检查类别分布\n",
    "print(train_df[\"Label\"].value_counts())\n",
    "\n",
    "# 特征和标签拆分\n",
    "X = small_df.drop(columns=[\"Label\"])\n",
    "y = small_df[\"Label\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"Label\"])  # 使用完整数据集适配编码器\n",
    "\n",
    "\n",
    "# 数据划分\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "# 对训练集和测试集标签进行编码\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train1)\n",
    "y_test_encoded = label_encoder.transform(y_test1)\n",
    "# 训练 baseline 模型（FT transformer）\n",
    "X_train_tensor1 = torch.tensor(X_train1.values, dtype=torch.float32)\n",
    "y_train_tensor1 = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor1 = torch.tensor(X_test1.values, dtype=torch.float32)\n",
    "y_test_tensor1 = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# 创建DataLoader\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset1 = TabularDataset(X_train_tensor1, y_train_tensor1)\n",
    "test_dataset1 = TabularDataset(X_test_tensor1, y_test_tensor1)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
    "num_features = X_train1.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = FTTransformer(num_features, num_classes)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# 检查模型参数所在的设'\n",
    "print(f\"num_classes: {num_classes}\")\n",
    "train_model(model, train_loader1, epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (606979458.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31munique_labels=[0 8 9 13 14]\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------- 标签编码 ----------\n",
    "# 合并所有标签（包括合成数据）\n",
    "all_labels = pd.concat([train_df[\"Label\"], final_synthetic_data[\"Label\"]], ignore_index=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)  # 适配所有可能的标签\n",
    "\n",
    "# 对训练集标签编码\n",
    "y_encoded = label_encoder.transform(train_df[\"Label\"])\n",
    "\n",
    "# 验证标签范围\n",
    "unique_labels = np.unique(y_encoded)\n",
    "unique_labels=[0, 8 ,9, 13 ,14]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"唯一标签值:\", unique_labels)\n",
    "print(\"模型输出类别数:\", num_classes)\n",
    "assert unique_labels.max() < num_classes, \"❌ 标签值超出模型输出范围\"\n",
    "\n",
    "# ---------- 模型初始化 ----------\n",
    "model = FTTransformer(num_features, num_classes)\n",
    "model = model.to(device)  # 确保模型在GPU（如果可用）\n",
    "\n",
    "# ---------- 数据加载 ----------\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.features[idx].to(device),  # 自动移至设备\n",
    "            self.labels[idx].to(device)\n",
    "        )\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LabelEncoder 识别的类别: ['BENIGN' 'Heartbleed' 'Infiltration' 'Web Attack � Sql Injection'\n",
      " 'Web Attack � XSS']\n",
      "✅ LabelEncoder 识别的类别数: 5\n",
      "⚠️ y_true 真实标签类别: {np.int64(0), np.int64(8), np.int64(9), np.int64(13), np.int64(14)}\n",
      "⚠️ y_pred 预测的唯一类别: {np.int64(2), np.int64(4)}\n",
      "📊 分类报告:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 7, does not match size of target_names, 5. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📉 混淆矩阵:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, label_encoder)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m模型预测出 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(invalid_preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m，但应在 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 范围内！\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 分类报告:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📉 混淆矩阵:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(y_true, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\戴尔\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2693\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2687\u001b[39m         warnings.warn(\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2689\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2690\u001b[39m             )\n\u001b[32m   2691\u001b[39m         )\n\u001b[32m   2692\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2693\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2697\u001b[39m         )\n\u001b[32m   2698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2699\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 7, does not match size of target_names, 5. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, label_encoder):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # 🚨 检查类别范围\n",
    "    print(\"✅ LabelEncoder 识别的类别:\", label_encoder.classes_)\n",
    "    print(\"✅ LabelEncoder 识别的类别数:\", len(label_encoder.classes_))\n",
    "    print(\"⚠️ y_true 真实标签类别:\", set(y_true))\n",
    "    print(\"⚠️ y_pred 预测的唯一类别:\", set(y_pred))\n",
    "\n",
    "    # 🚨 强制检查 y_pred 是否超出范围\n",
    "    valid_labels = list(range(len(label_encoder.classes_)))  # [0, 1, 2, 3, 4]\n",
    "    invalid_preds = [label for label in y_pred if label not in valid_labels]\n",
    "\n",
    "    if invalid_preds:\n",
    "        print(f\"❌ 发现 {len(invalid_preds)} 个无效预测类别: {set(invalid_preds)}\")\n",
    "        raise ValueError(f\"模型预测出 {set(invalid_preds)}，但应在 {valid_labels} 范围内！\")\n",
    "\n",
    "    print(\"📊 分类报告:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
    "    print(\"📉 混淆矩阵:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "evaluate_model(model, test_loader1, label_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 创建 SHAP 解释器\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# 计算 SHAP 值\n",
    "shap_values = explainer(X_train1,check_additivity=False)\n",
    "\n",
    "# 可视化 SHAP 值（全局）\n",
    "shap.summary_plot(shap_values, X_train1) \n",
    "# 可视化 SHAP 值（局部）\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_train1[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
